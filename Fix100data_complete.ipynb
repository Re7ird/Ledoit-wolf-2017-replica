{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a951f0-75d6-4918-8b58-06dd1eb69251",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Answer Sheet for Question 3\n",
    "\n",
    "In this file, I used a fixed example to replicate Table 1 of ledoit and wolf(2017) paper. I cannot find the history stock list of the S&P 500 and the current components of the S&P500 do not have all the data I need from 1 Jan 1972 to 31 Dec 2011 (10094 trading days). There are only 26 stocks that have complete historical data of 10094 days, so I only use 1 time period of 271 returns.\n",
    "\n",
    "I use yfinance package to get the stock data and use numpy, pandas, sklearn to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8beb15-fe20-45e5-aa06-9a957f7ebb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def showmatrixinfo(matrix):\n",
    "    print(\"Matrix shape: \",np.shape(matrix))\n",
    "    print(matrix)\n",
    "\n",
    "    #######This function is only for testing purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97197305-aeef-4abd-97de-748e4fa3f731",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stock selection and data acquisition\n",
    "\n",
    "I randomly pick 100 stocks for N=100 and save it to a csv file, then read and request the data from 1997-3-5 to 1998-4-1. If I cannot get full historical data of 272 days of one stock, I will pop out this stock and randomly replace it with another one.\n",
    "\n",
    "The time period \"1997-3-5 to 1998-4-1\" was picked by me manually. I looked at the historical S&P 500 index and the trend seems steady in this period. so I can assume that the distributions in these 272 days remain the same.\n",
    "\n",
    "This part(yfinance) requires internet connection. yfinance seems not working in the Chinese mainland. I actually used SCRP of the department to do all the coding. I run this file, save data, and download it to my local computer to double check. If yfinance is not available, you can jump to the next part after loading the return data which is already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f537bf5d-9472-40dd-a7ee-5ab7389428a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BA', 'BMY', 'CPB', 'CAT', 'CVX', 'KO', 'CL', 'COP', 'CVS', 'DE', 'DTE', 'EIX', 'ETR', 'EXC', 'XOM', 'F', 'GD', 'HAL', 'HIG', 'HSY', 'IBM', 'IP', 'KMB', 'KR', 'MRK', 'NSC', 'PEP', 'PFE', 'PPG', 'PG', 'PEG', 'SEE', 'SO', 'UNP', 'XEL', 'ABT', 'HON', 'AEP', 'SHW', 'CMI', 'EMR', 'SLB', 'CSX', 'CLX', 'GIS', 'NEM', 'MCD', 'LLY', 'BAX', 'BDX', 'JNJ', 'GPC', 'HPQ', 'WMB', 'JPM', 'IFF', 'AXP', 'BAC', 'CI', 'DIS', 'DUK', 'LNC', 'TAP', 'NEE', 'WFC', 'MMM', 'INTC', 'TGT', 'TXT', 'VFC', 'WBA', 'AIG', 'FDX', 'PCAR', 'ADP', 'MAS', 'GWW', 'ADM', 'WMT', 'SNA', 'SWK', 'MO', 'AAPL', 'OXY', 'CAG', 'BBWI', 'VZ', 'T', 'LOW', 'PHM', 'HES', 'LMT', 'HAS', 'BALL', 'APD', 'NUE', 'PKI', 'NOC', 'CNP', 'TJX']\n"
     ]
    }
   ],
   "source": [
    "def openfile(filename):\n",
    "    a=open(filename,'r')\n",
    "    b=a.readlines()\n",
    "    a.close()\n",
    "    return b\n",
    "\n",
    "stock=openfile(\"Fix100comp.csv\")\n",
    "stocklist=stock[0].replace(\"\\n\",\"\").split(\",\")\n",
    "print(stocklist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e626e438-0088-4cfd-a8f1-307822da600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "                Open     High       Low   Close  Adj Close    Volume\n",
      "Date                                                                \n",
      "1999-01-05  3.554688  3.71875  3.546875  3.6875   2.816108  10439200\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Date\n",
      "1999-01-05    2.816108\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "########This is only for test. If yfinance is not available, you can jump to next part after loading the already processed return data.\n",
    "stock=stocklist[0]\n",
    "print(yf.download(i, start=\"1999-1-5\", end=\"1999-1-6\"))                                            ##test\n",
    "print(yf.download(i, start=\"1999-1-5\", end=\"1999-1-6\").iloc[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25906323-7497-446f-90d5-297d9ebf7216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "(272, 6)\n"
     ]
    }
   ],
   "source": [
    "########Check data availability\n",
    "for i in stocklist:\n",
    "    data = yf.download(i, start=\"1997-3-5\", end=\"1998-4-2\")\n",
    "    print(data.shape)   #Check data availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c1aee2-34f6-4315-b75a-9f58317d0394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "######Generate price matrix(N,272),this will yields 271 returns from 1997-3-6 to 1998-4-1. yfinance will not return the data of the ending date.\n",
    "\n",
    "Pr=list()\n",
    "for i in stocklist:\n",
    "    data = yf.download(i, start=\"1997-3-5\", end=\"1998-4-2\")\n",
    "    ls=list(data.iloc[:,4])\n",
    "    Pr.append(ls)\n",
    "Pr=np.array(Pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "3cab6543-039e-4c5d-a497-e95eea38ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 272)\n",
      "32.631103515625\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Pr))        #check\n",
    "print(Pr[0,0])  ##Correct\n",
    "\n",
    "#Save file.\n",
    "np.savetxt(\"Price.txt\", Pr, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c12fa-fbf6-44b9-b71f-c2192568d0dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data processing and calculating returns.\n",
    "First, I calculate the return matrix of 271 days, then divide it into two parts: 270 days for estimating parameters, and 21 days for testing the results.\n",
    "\n",
    "The risk-free rate is from Ken French's Data Library, the same as FAMA-FRENCH covariance estimates. In the paper by Ledoit and Wolf(2017), they annualize returns linearly instead of using compound interest. So, I simply divided the risk-free rate by 250 to calculate the daily risk-free rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4e1204e-c6b2-4542-b5cf-1a794e4a0af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00238916  0.01916199 -0.00235089 ... -0.01463477  0.03217799\n",
      "   0.01199075]\n",
      " [-0.03610098  0.          0.01685407 ...  0.00678584  0.02267097\n",
      "   0.02119442]\n",
      " [ 0.01386943  0.00684016  0.02309794 ...  0.0042062  -0.00711211\n",
      "  -0.01652012]\n",
      " ...\n",
      " [ 0.00851838  0.01013458  0.00334456 ...  0.00120744  0.03679161\n",
      "   0.01803369]\n",
      " [-0.0374327   0.          0.         ...  0.00668137  0.0176998\n",
      "   0.0021737 ]\n",
      " [ 0.          0.00284913  0.         ... -0.03137801  0.02676057\n",
      "   0.03017811]]\n"
     ]
    }
   ],
   "source": [
    "Y1=Pr[:,1:]               ##Create a matrix only 271 days(day2-day272).\n",
    "Y2=Pr[:,:-1]               ##day1 to day271\n",
    "Yt=Y1/Y2-1\n",
    "print(Yt)                   #Yt is the return matrix of 100 stocks(271days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "9227ebfc-83df-4ed9-8dac-e56ccc1b8bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 250)\n",
      "(100, 21)\n"
     ]
    }
   ],
   "source": [
    "yt_est=Yt[:,:250]                     #yt for estimating covariance matrix(250days)\n",
    "yt_test=Yt[:,250:]                    #yt for testing the portfolio(21days)\n",
    "print(np.shape(yt_est))               \n",
    "print(np.shape(yt_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6bc20eeb-dbee-4d65-819f-ea2fe4c51ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save data\n",
    "\n",
    "np.save(\"Fix100Stock_est_matrix.npy\",yt_est) #save matrix\n",
    "np.save(\"Fix100Stock_test_matrix.npy\",yt_test)\n",
    "np.savetxt(\"Fix100_Yt_est.csv\",yt_est,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a3e3d-b6e1-4f57-a7e0-429a9317cb65",
   "metadata": {},
   "source": [
    "### All the codes below do not need internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f1e3c6-96df-408e-af3c-fe315ca36850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 250)\n",
      "(100, 21)\n",
      "(21,)\n",
      "[[ 0.01789964 -0.01992981  0.00358893 ... -0.01463477  0.03217799\n",
      "   0.01199075]\n",
      " [-0.01161362 -0.01051343  0.02499983 ...  0.00678584  0.02267097\n",
      "   0.02119442]\n",
      " [-0.00213677 -0.00535299  0.01399348 ...  0.0042062  -0.00711211\n",
      "  -0.01652012]\n",
      " ...\n",
      " [-0.00507116  0.00111554  0.02228419 ...  0.00120744  0.03679161\n",
      "   0.01803369]\n",
      " [ 0.          0.00722904  0.01435389 ...  0.00668137  0.0176998\n",
      "   0.0021737 ]\n",
      " [-0.01433089 -0.00484653  0.01623402 ... -0.03137801  0.02676057\n",
      "   0.03017811]]\n"
     ]
    }
   ],
   "source": [
    "#Load and check data\n",
    "\n",
    "yt_est=np.load(\"Fix100Stock_est_matrix.npy\")\n",
    "yt_test=np.load(\"Fix100Stock_test_matrix.npy\")\n",
    "\n",
    "FFdata = pd.read_csv('F-F_Research_Data_Factors_daily_Fix100.csv')\n",
    "rf_rate=FFdata.loc[250:,\"RF\"].to_numpy()\n",
    "rf_daily=rf_rate/250                          \n",
    "\n",
    "\n",
    "print(np.shape(yt_est))               \n",
    "print(np.shape(yt_test)) \n",
    "print(np.shape(rf_daily))\n",
    "print(yt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d3a6b-094f-41a8-b7e1-f461cded9a8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Covariance Matrix Estimation\n",
    "\n",
    "I used different approaches to estimate the covariance matrix then define a function \"test_result\" to calculate the portfolio weight w, the return of 21 days and other estimators, then save it to a list.\n",
    "\n",
    "### 1/N\n",
    "This is 1/N approach. It assumes that every stock is not correlated. The covariance matrix is set to be I(N, N), which will yield equal portfolio weight as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348e0d2f-84ef-4fb3-b81d-b8c466f809d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "w:\n",
      "[[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      "  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      "  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      "  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      "  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      "  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      "  0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      "  0.01 0.01]]\n",
      "(1, 21)\n",
      "\n",
      "1/N result:\n",
      "AV: 54.8711\n",
      "SD: 8.5095\n",
      "SR: 6.4482\n",
      "[54.87108271762652, 8.509538561633054, 6.448185447448784]\n"
     ]
    }
   ],
   "source": [
    "##     1/N Estimation\n",
    "\n",
    "N=100\n",
    "omega=np.identity(100)\n",
    "print(omega)\n",
    "\n",
    "def test_result(omega,method):\n",
    "    ### Use estimated covariance matrix to calculate w\n",
    "    \n",
    "    Omega= np.matrix(omega)\n",
    "    w=(Omega.I@np.ones((100,1)))/(np.ones((1,100))@Omega.I@np.ones((100,1)))\n",
    "    print(\"w:\")\n",
    "    print(w.T)\n",
    "    \n",
    "    ###########calculate payoff and excess payoff(ri-rf)\n",
    "    \n",
    "    payoff=w.T@yt_test*100\n",
    "    \n",
    "    exc_payoff=payoff-rf_daily\n",
    "    print(np.shape(exc_payoff))\n",
    "    ############Annualize:\n",
    "    \n",
    "    AV=exc_payoff.mean()*250\n",
    "    SD=exc_payoff.std()*pow(250,.5)\n",
    "    SR=AV/SD\n",
    "\n",
    "    print(\"\\n\"+method+\" result:\")\n",
    "    print(\"AV: {:.4f}\".format(AV))\n",
    "    print(\"SD: {:.4f}\".format(SD))\n",
    "    print(\"SR: {:.4f}\".format(SR))\n",
    "    return [AV,SD,SR]\n",
    "\n",
    "result1N=test_result(omega,\"1/N\")\n",
    "print(result1N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae85c7-686a-4e97-9e6f-688d90558228",
   "metadata": {},
   "source": [
    "### Sample\n",
    "This is the Sample covariance matrix approach, the classical method for Markowitz's MPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2be9b96d-3ccd-49a3-8fd6-d50eb7bd0487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape:\n",
      "(100, 100)\n",
      "[[4.65997962e-04 1.63687852e-04 1.32154780e-04 ... 1.20673976e-04\n",
      "  6.87962711e-05 1.69091373e-04]\n",
      " [1.63687852e-04 5.05786101e-04 2.03311377e-04 ... 1.04776241e-04\n",
      "  8.67511550e-05 1.68726755e-04]\n",
      " [1.32154780e-04 2.03311377e-04 3.68006616e-04 ... 1.01790305e-04\n",
      "  5.12305790e-05 1.23077641e-04]\n",
      " ...\n",
      " [1.20673976e-04 1.04776241e-04 1.01790305e-04 ... 4.23508458e-04\n",
      "  3.99773908e-05 9.01986996e-05]\n",
      " [6.87962711e-05 8.67511550e-05 5.12305790e-05 ... 3.99773908e-05\n",
      "  2.24052101e-04 5.58509147e-05]\n",
      " [1.69091373e-04 1.68726755e-04 1.23077641e-04 ... 9.01986996e-05\n",
      "  5.58509147e-05 5.87914599e-04]]\n",
      "w:\n",
      "[[ 0.00782784 -0.00789738 -0.09516237  0.01701728  0.02315577 -0.05797588\n",
      "   0.00686327  0.02343564  0.00105536  0.00082359  0.13517302 -0.05073402\n",
      "   0.01183255  0.0154362  -0.07560041  0.01889112  0.04293443  0.03592357\n",
      "   0.04300626 -0.02095569  0.01702989 -0.04232427 -0.00591592 -0.00339875\n",
      "   0.0185084  -0.05853328 -0.00148419 -0.00536898 -0.00245309 -0.02344836\n",
      "   0.03318052  0.09043834 -0.02151486  0.06562489  0.1115352  -0.05136415\n",
      "  -0.05209073  0.16755467 -0.01787846 -0.0111262   0.03555239 -0.05952679\n",
      "   0.01215287  0.05029678  0.10180839  0.01085416  0.0552036  -0.01624072\n",
      "   0.01607923 -0.05519632 -0.00199431  0.08829938 -0.0158331   0.00021565\n",
      "   0.04885882  0.04601749 -0.0852429  -0.04324233  0.00321163  0.04825594\n",
      "   0.05032211  0.01303825  0.01258747  0.19318203 -0.11238622  0.06550619\n",
      "   0.02256591 -0.01247741 -0.02235849  0.04075087  0.02024801 -0.00945258\n",
      "  -0.00041934 -0.01756989  0.0202482   0.05656775  0.04895535  0.05053103\n",
      "   0.03005697  0.06873219 -0.0474797  -0.05031258  0.01740403  0.03762351\n",
      "   0.00741378 -0.02364179  0.0303726  -0.08896878  0.01532694  0.02138778\n",
      "  -0.01201683  0.04369983  0.02430442 -0.00785203  0.05518467  0.01091834\n",
      "   0.00777949  0.03403699 -0.08633722 -0.02902251]]\n",
      "(1, 21)\n",
      "\n",
      "Samp result:\n",
      "AV: 55.5981\n",
      "SD: 10.2866\n",
      "SR: 5.4049\n",
      "[55.598106763739665, 10.286575290190576, 5.404919051801313]\n"
     ]
    }
   ],
   "source": [
    "##     Sample Covariance Estimation\n",
    "omega=np.cov(yt_est)\n",
    "print(\"Matrix shape:\")\n",
    "print(np.shape(omega))\n",
    "print(omega)\n",
    "\n",
    "resultSamp=test_result(omega,\"Samp\")\n",
    "print(resultSamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267b759-95ac-4326-8d0a-74b87aa05ee7",
   "metadata": {},
   "source": [
    "### Lin\n",
    "This is the linear shrinkage approach. The matrix is given by the linear shrinkage estimator of Ledoit and Wolf(2004). While I googled the method, I found out that sklern package has this method, which will return the matrix and coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "939a1743-4446-418a-ab7c-0fd2471daf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape:\n",
      "(100, 100)\n",
      "[[4.54553244e-04 1.49775906e-04 1.20922852e-04 ... 1.10417810e-04\n",
      "  6.29492276e-05 1.54720178e-04]\n",
      " [1.49775906e-04 4.90959761e-04 1.86031800e-04 ... 9.58712347e-05\n",
      "  7.93781133e-05 1.54386550e-04]\n",
      " [1.20922852e-04 1.86031800e-04 3.64890251e-04 ... 9.31390750e-05\n",
      "  4.68764560e-05 1.12617186e-04]\n",
      " ...\n",
      " [1.10417810e-04 9.58712347e-05 9.31390750e-05 ... 4.15674953e-04\n",
      "  3.65796842e-05 8.25326486e-05]\n",
      " [6.29492276e-05 7.93781133e-05 4.68764560e-05 ... 3.65796842e-05\n",
      "  2.33170532e-04 5.11041061e-05]\n",
      " [1.54720178e-04 1.54386550e-04 1.12617186e-04 ... 8.25326486e-05\n",
      "  5.11041061e-05 5.66108100e-04]]\n",
      "w:\n",
      "[[ 0.00494146 -0.01270759 -0.06927698  0.00851496  0.01040177 -0.04685151\n",
      "   0.00359105  0.03074149  0.01736969  0.00644099  0.08450095 -0.01903434\n",
      "   0.02569077  0.02607242 -0.06068882  0.0170703   0.04465801  0.02245166\n",
      "   0.03334027 -0.00903853  0.01280188 -0.03183679 -0.0098643   0.00321426\n",
      "   0.02038573 -0.03732696 -0.00243112 -0.01685952 -0.01737469 -0.01633966\n",
      "   0.04288008  0.08113585  0.0129613   0.05799822  0.08605447 -0.03668831\n",
      "  -0.04998557  0.1192352  -0.01183035 -0.0060419   0.0279789  -0.04264014\n",
      "  -0.00457961  0.0418138   0.08401007  0.01666391  0.04487457 -0.01292215\n",
      "   0.01039077 -0.03697024 -0.00217961  0.07371464 -0.02052724  0.00660365\n",
      "   0.02643957  0.03827219 -0.05413801 -0.02888149  0.00699141  0.03513641\n",
      "   0.05802357  0.00898084  0.01225911  0.12716319 -0.08771622  0.06183256\n",
      "   0.02044536 -0.0230815  -0.02445738  0.04023846  0.00809409 -0.01846113\n",
      "  -0.00710368 -0.01924176  0.01954283  0.05453143  0.04908636  0.04420414\n",
      "   0.02237832  0.05212305 -0.03662288 -0.04694079  0.01974826  0.03423801\n",
      "  -0.00386326 -0.00582372  0.02352845 -0.05909679  0.01841432  0.01028742\n",
      "  -0.00460075  0.05398129  0.02727447  0.00519751  0.05710414  0.01526284\n",
      "   0.00338475  0.0280039  -0.0463151  -0.02033095]]\n",
      "(1, 21)\n",
      "\n",
      "Lin result:\n",
      "AV: 61.4428\n",
      "SD: 8.4146\n",
      "SR: 7.3019\n",
      "[61.4427844131374, 8.414576989113774, 7.301945717845119]\n"
     ]
    }
   ],
   "source": [
    "##     Lin(Ledoit-Wolf 2004 methpd) shrinkage Estimation\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "\n",
    "omega=ledoit_wolf(yt_est.T)[0]\n",
    "print(\"Matrix shape:\")\n",
    "print(np.shape(omega))\n",
    "print(omega)\n",
    "\n",
    "resultLin=test_result(omega,\"Lin\")\n",
    "print(resultLin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24f6d2-1951-4798-9a84-19a61efcd9ad",
   "metadata": {},
   "source": [
    "### NonLin\n",
    "This is the Non-Linear shrinkage estimate procedure. I followed the procedure of the reference paper. The estimator of s(x) is tricky. I cannot find the exact answer to it. The appendix of the paper provides more details but still, it's too obscure to me. \n",
    "Then I found a package that will generate the covariance matrix estimator which is nonlinshrink. In this file I used the package to complete the empirical test of 50 stocks of fixed universe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7fe5566-887f-42d9-ada3-6ce5f554fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "w:\n",
      "[[-2.08235840e-03 -1.49117724e-02 -4.58887462e-02  7.59500347e-04\n",
      "  -3.98066354e-03 -2.95287211e-02 -5.20878601e-03  3.67912958e-02\n",
      "   2.86775479e-02  2.47029881e-03  6.02696205e-02  1.62490324e-02\n",
      "   4.20800981e-02  3.92252166e-02 -3.09897874e-02  1.21836461e-02\n",
      "   4.44060815e-02  6.23836569e-03  2.64221207e-02 -4.86546682e-03\n",
      "   5.27870076e-03 -1.33799264e-02 -1.56713077e-02  5.97270779e-03\n",
      "   1.19722317e-02 -1.64232919e-02 -1.15778266e-02 -2.21877774e-02\n",
      "  -1.01592651e-02 -2.15025487e-02  4.15448017e-02  7.40201028e-02\n",
      "   3.07266694e-02  5.33849942e-02  6.67450866e-02 -1.44626203e-02\n",
      "  -4.28058856e-02  8.74095754e-02 -3.72769336e-05  1.16189035e-03\n",
      "   1.51258930e-02 -2.57261026e-02 -6.36458311e-03  3.17065613e-02\n",
      "   6.23178059e-02  2.64910485e-02  3.29258015e-02 -1.51650623e-02\n",
      "   1.66519314e-04 -1.99452611e-02 -1.26762807e-03  4.90037324e-02\n",
      "  -1.38174640e-02  1.86018886e-02 -1.78641453e-03  3.17765550e-02\n",
      "  -4.18618332e-02 -1.45766502e-02  1.61004998e-02  1.31870350e-02\n",
      "   3.86107799e-02  1.05794477e-02  8.09789628e-03  9.78824375e-02\n",
      "  -5.60142077e-02  5.05497053e-02  1.28642138e-02 -3.24504234e-02\n",
      "  -1.63331159e-02  4.11692964e-02 -6.98977650e-03 -2.12305132e-02\n",
      "  -1.54914345e-02 -2.57777746e-02  1.62317783e-02  5.24952918e-02\n",
      "   3.27828241e-02  3.73683406e-02  1.34487649e-02  3.06876660e-02\n",
      "  -1.37084477e-02 -4.33207003e-02  2.29993748e-02  3.60522350e-02\n",
      "  -1.91863608e-02  8.91580058e-03  2.07181330e-02 -1.91072354e-02\n",
      "   1.91368949e-02 -1.66629697e-03  5.60224888e-03  4.87242071e-02\n",
      "   3.11281673e-02  1.58005015e-02  4.48295982e-02  1.31070545e-02\n",
      "   2.44016350e-03  2.60165995e-02  3.67022584e-03 -1.58512580e-02]]\n",
      "(1, 21)\n",
      "\n",
      "NonLin result:\n",
      "AV: 73.7365\n",
      "SD: 7.1009\n",
      "SR: 10.3842\n"
     ]
    }
   ],
   "source": [
    "##     NonLin shrinkage Estimation\n",
    "import nonlinshrink as nls\n",
    "\n",
    "omega=nls.shrink_cov(yt_est.T)\n",
    "print(np.shape(omega))\n",
    "\n",
    "resultNonLin=test_result(omega,\"NonLin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2fbcc-d7b3-4e5b-8f3e-d63486f9c444",
   "metadata": {},
   "source": [
    "### SF\n",
    "This is the single-factor model. First, I generate factors using an equal-weighted portfolio. Then, I calculate the factor variance and covariance of the factor and single stock. I also saved the data and run it in STATA to check if I'm right. The covariance of stock i and j is equal to $ \\frac{beta1 * beta2}{var(factor)} $ if $ i \\neq j $, the diagonals need to be replaced by var(ri) or add the variance of residuals.  (I'm not familiar with LATEX, still working on it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c16b84ed-21e0-4032-84c8-329a58a27fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equal portfolio:  (1, 100)\n",
      "Factor matrix shape:  (1, 250)\n",
      "Matrix shape:  (100, 100)\n",
      "[[4.65997962e-04 1.82112016e-04 1.64998702e-04 ... 1.01471267e-04\n",
      "  7.28569689e-05 1.56005464e-04]\n",
      " [1.82112016e-04 5.05786101e-04 1.92200615e-04 ... 1.18199960e-04\n",
      "  8.48682692e-05 1.81724739e-04]\n",
      " [1.64998702e-04 1.92200615e-04 3.68006616e-04 ... 1.07092549e-04\n",
      "  7.68930823e-05 1.64647818e-04]\n",
      " ...\n",
      " [1.01471267e-04 1.18199960e-04 1.07092549e-04 ... 4.23508458e-04\n",
      "  4.72878781e-05 1.01255480e-04]\n",
      " [7.28569689e-05 8.48682692e-05 7.68930823e-05 ... 4.72878781e-05\n",
      "  2.24052101e-04 7.27020323e-05]\n",
      " [1.56005464e-04 1.81724739e-04 1.64647818e-04 ... 1.01255480e-04\n",
      "  7.27020323e-05 5.87914599e-04]]\n",
      "w:\n",
      "[[-0.01158836 -0.02306372 -0.02402899 -0.00370027 -0.00331666 -0.0299433\n",
      "  -0.01534953  0.02758373  0.00984259 -0.00257735  0.08203581  0.04367178\n",
      "   0.06361238  0.03669402 -0.01863309  0.0049561   0.03350595 -0.00591081\n",
      "   0.01912271 -0.00575308 -0.01568702 -0.00409937 -0.00500743  0.00762265\n",
      "  -0.01274834 -0.01112693 -0.0104716  -0.03266031 -0.0056785  -0.0251494\n",
      "   0.07113103  0.0227373   0.06817781  0.02821926  0.12569628 -0.0125684\n",
      "  -0.02696771  0.16711027 -0.00379695 -0.00452559  0.00108262 -0.01267278\n",
      "  -0.00205376  0.02274327  0.06927911  0.00744027  0.0192874  -0.0236994\n",
      "  -0.02205936 -0.0112524  -0.01448911  0.03565715 -0.01142122  0.02153278\n",
      "  -0.01652303  0.01408569 -0.03188988 -0.01666446  0.01862662 -0.00602953\n",
      "   0.08476192  0.01607915  0.00425212  0.15286387 -0.03632339  0.01868619\n",
      "  -0.00961972 -0.0229491  -0.00597597  0.03630947 -0.01187847 -0.02507079\n",
      "  -0.0103294  -0.01505605  0.00622211  0.03487612  0.03597644  0.0128161\n",
      "  -0.0057939   0.02917841 -0.00704398 -0.01203856  0.00284533  0.03041384\n",
      "  -0.01559903  0.01450696  0.02104634  0.01216961  0.00654439 -0.01113545\n",
      "   0.0097932   0.03391086  0.00973953  0.00800219  0.03021892  0.00135131\n",
      "   0.00181326  0.00891934  0.03537684 -0.00820694]]\n",
      "(1, 21)\n",
      "\n",
      "SF result:\n",
      "AV: 94.5516\n",
      "SD: 10.0423\n",
      "SR: 9.4153\n"
     ]
    }
   ],
   "source": [
    "##     Single factor estimation\n",
    "##   First, estimate SigmaF=Cov(betai,betaj)\n",
    "\n",
    "##   Generate Factor\n",
    "equalw=np.array([[0.01]*100])\n",
    "print(\"equal portfolio: \",np.shape(equalw))\n",
    "factor=equalw@yt_est\n",
    "print(\"Factor matrix shape: \",np.shape(factor))\n",
    "\n",
    "\n",
    "##estimate SigmaF\n",
    "var_f=np.var(factor,ddof=1)      #variance of factor,\n",
    "\n",
    "np.savetxt(\"SF_factor.csv\",factor,delimiter=\",\")\n",
    "\n",
    "##compute cov(Ri,Rf),the covariance of stocks and factor\n",
    "var_if=np.cov(yt_est,factor)[-1,:-1]\n",
    "var_if=np.matrix(var_if)          #convert to 1X100 matrix\n",
    "\n",
    "\n",
    "\n",
    "SigmaSF=var_if.T*var_if/var_f  \n",
    "\n",
    "##The elements of diagonal need to change to var(ri) ,or betai^2*var_f + var(residual)\n",
    "\n",
    "for i in range(100):\n",
    "    SigmaSF[i,i]=np.cov(yt_est)[i,i]\n",
    "\n",
    "\n",
    "showmatrixinfo(SigmaSF)    ####Seems correct\n",
    "\n",
    "\n",
    "resultSF=test_result(SigmaSF,\"SF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec633a-0995-4d9c-bd9c-4a8829f057fc",
   "metadata": {},
   "source": [
    "### FF\n",
    "In the Fama and French model, I also downloaded the data from Ken French's Data Library as Ledoit and Wolf did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7d0a354-2f31-43ef-aa63-6d9a802e146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta matrix:  (100, 3)\n",
      "SigmaF: \n",
      "Matrix shape:  (100, 100)\n",
      "[[4.65997962e-04 1.81657701e-04 1.58188346e-04 ... 9.14582713e-05\n",
      "  6.18714905e-05 1.49341589e-04]\n",
      " [1.81657701e-04 5.05786101e-04 1.88232657e-04 ... 1.09899032e-04\n",
      "  7.61206669e-05 1.69057593e-04]\n",
      " [1.58188346e-04 1.88232657e-04 3.68006616e-04 ... 9.76882145e-05\n",
      "  6.98722984e-05 1.50011922e-04]\n",
      " ...\n",
      " [9.14582713e-05 1.09899032e-04 9.76882145e-05 ... 4.23508458e-04\n",
      "  4.13773752e-05 8.71241591e-05]\n",
      " [6.18714905e-05 7.61206669e-05 6.98722984e-05 ... 4.13773752e-05\n",
      "  2.24052101e-04 6.03188208e-05]\n",
      " [1.49341589e-04 1.69057593e-04 1.50011922e-04 ... 8.71241591e-05\n",
      "  6.03188208e-05 5.87914599e-04]]\n",
      "w:\n",
      "[[-0.00585698 -0.02128737 -0.02309375 -0.00118393 -0.00131329 -0.03139742\n",
      "  -0.01006522  0.02712724  0.00972651  0.003003    0.07766524  0.03425162\n",
      "   0.06072673  0.03405158 -0.01084858 -0.00914393  0.03725362 -0.00071683\n",
      "   0.00899339 -0.00621549 -0.00516377 -0.00790941 -0.00377452  0.00597547\n",
      "  -0.01203261  0.00153645 -0.00334065 -0.02617199 -0.00901608 -0.01499144\n",
      "   0.06541306  0.0209107   0.05724226  0.02768586  0.11812905 -0.00480978\n",
      "  -0.01731636  0.1525883  -0.00204015 -0.00048198 -0.00386688 -0.00662922\n",
      "   0.0019866   0.01761173  0.07379777  0.01347429  0.02322043 -0.01993699\n",
      "  -0.02128524 -0.00939122 -0.01747465  0.04431451  0.00247952  0.0277388\n",
      "  -0.04594474  0.01073434 -0.04969723 -0.04262159  0.01475807 -0.00281632\n",
      "   0.08078883 -0.00581069  0.0072427   0.14409214 -0.05852004  0.02037121\n",
      "   0.00823767 -0.01815945 -0.00790817  0.03002153 -0.00632484 -0.04301393\n",
      "  -0.00746918 -0.01563264  0.0180706   0.03364858  0.03492973  0.02018941\n",
      "  -0.00068004  0.03098395 -0.00351805 -0.00659201  0.00698275  0.02779652\n",
      "  -0.01073291  0.02505041  0.02085031  0.01044931  0.00742803 -0.01478517\n",
      "   0.01821873  0.0372308   0.00774262  0.01181999  0.02414934  0.0054596\n",
      "   0.00462697  0.0094911   0.0322994  -0.00358569]]\n",
      "(1, 21)\n",
      "\n",
      "FF result:\n",
      "AV: 89.1074\n",
      "SD: 8.6302\n",
      "SR: 10.3250\n"
     ]
    }
   ],
   "source": [
    "##     FAMA FRENCH estimation\n",
    "\n",
    "#####First, Generate 3-factors array.\n",
    "import pandas as pd\n",
    "FFdata = pd.read_csv('F-F_Research_Data_Factors_daily_Fix100.csv')\n",
    "index=[\"Mkt-RF\",\"SMB\",\"HML\"]\n",
    "FFfactors=FFdata.loc[:249,index].to_numpy()\n",
    "#showmatrixinfo(FFfactors)                #####Done\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LG=LinearRegression()\n",
    "\n",
    "LG.fit(FFfactors,yt_est.T)      ##FFfactor matirx is a (250,3) matrix!\n",
    "betas=LG.coef_\n",
    "print(\"Beta matrix: \",np.shape(betas))\n",
    "\n",
    "var_ff=np.cov(FFfactors.T)      ##Covariance of FAMA FRENCH 3 Factor model.\n",
    "\n",
    "SigmaF=betas@var_ff@betas.T\n",
    "\n",
    "###As same as SF, the diagonal need add residual,or replace by var(Ri)\n",
    "\n",
    "for i in range(100):\n",
    "    SigmaF[i,i]=np.cov(yt_est)[i,i]\n",
    "    \n",
    "print(\"SigmaF: \")\n",
    "showmatrixinfo(SigmaF)\n",
    "\n",
    "resultFF=test_result(SigmaF,\"FF\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9febc-5068-4cbb-ba25-d060ac8a18d7",
   "metadata": {},
   "source": [
    "### POET\n",
    "This is the POET estimation of covariance matrix by Fan et al. (2013). I use PCA from sklearn to generate 5 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cd2f9a2-40dc-46e8-bdf1-fc6f681b11d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating components:\n",
      "PCA Componets:\n",
      " [[-0.03258095 -0.03888459 -0.0065898  ...  0.00327815 -0.06768189\n",
      "   0.00408442]\n",
      " [-0.02718246 -0.00858614 -0.06876589 ...  0.01374791  0.02649288\n",
      "  -0.01126418]\n",
      " [ 0.04822549 -0.05586383 -0.0338755  ...  0.01011372  0.05182732\n",
      "   0.05259222]\n",
      " [ 0.01281936  0.00725788  0.07087674 ... -0.06776106 -0.00534046\n",
      "  -0.00301675]\n",
      " [ 0.03553163  0.07364333  0.06746965 ... -0.0111177   0.09713746\n",
      "   0.07164584]]\n",
      "(5, 250)\n",
      "\n",
      "Eigenvalues: [0.00521385 0.00365524 0.00307917 0.00236818 0.00214257]\n",
      "Variance explaination: [0.08228226 0.05768501 0.04859386 0.03737338 0.03381289]\n",
      "Add up:  0.2597474094171566\n",
      "\n",
      "Regression:\n",
      "Beta matrix:  (100, 5)\n",
      "Matrix shape:  (100, 100)\n",
      "[[4.65997962e-04 1.63375681e-04 1.08053351e-04 ... 5.98541791e-05\n",
      "  6.68605577e-06 1.32445330e-04]\n",
      " [1.63375681e-04 5.05786101e-04 1.48057214e-04 ... 7.78681965e-05\n",
      "  2.23009885e-05 1.63153519e-04]\n",
      " [1.08053351e-04 1.48057214e-04 3.68006616e-04 ... 4.90447769e-05\n",
      "  9.45440735e-06 1.08309142e-04]\n",
      " ...\n",
      " [5.98541791e-05 7.78681965e-05 4.90447769e-05 ... 4.23508458e-04\n",
      "  8.26614955e-06 5.11021203e-05]\n",
      " [6.68605577e-06 2.23009885e-05 9.45440735e-06 ... 8.26614955e-06\n",
      "  2.24052101e-04 7.90819013e-06]\n",
      " [1.32445330e-04 1.63153519e-04 1.08309142e-04 ... 5.11021203e-05\n",
      "  7.90819013e-06 5.87914599e-04]]\n",
      "w:\n",
      "[[-0.00797665 -0.02009149 -0.00069322  0.00171859  0.0078706  -0.00671633\n",
      "  -0.00375301  0.02338729  0.00495079  0.00021987  0.05283407  0.03558326\n",
      "   0.04412857  0.02904249  0.00314141  0.00878285  0.01539529  0.00013134\n",
      "   0.01497504  0.00185569 -0.01632973  0.00304617  0.00386709  0.01109242\n",
      "  -0.01444013  0.00091128  0.00018804 -0.02440244  0.00981194 -0.00809328\n",
      "   0.04619979  0.01472856  0.05092595  0.01781768  0.06559776 -0.00291165\n",
      "  -0.00565659  0.0891456   0.00092774 -0.00225301  0.00460303 -0.00138788\n",
      "   0.00736266  0.01656993  0.03265493  0.02806141  0.01376147 -0.0198725\n",
      "  -0.0175616   0.00122025 -0.00475519  0.01968342 -0.00616616  0.01884074\n",
      "  -0.00260338  0.00831465 -0.00924576 -0.00566283  0.01271824  0.00193682\n",
      "   0.05208998  0.01529178  0.00188189  0.08601722 -0.00660276  0.01057174\n",
      "  -0.0145909  -0.00679053  0.00651399  0.02074697  0.00078935 -0.00112069\n",
      "  -0.00163832 -0.02123703  0.00825986  0.02398796  0.02080347  0.0105295\n",
      "  -0.00177774  0.02331131  0.00037729  0.00181266  0.02954241  0.02547347\n",
      "  -0.00041045  0.0151659   0.01847592  0.01620886  0.00850942 -0.00197754\n",
      "   0.01678682  0.02184979  0.00837406  0.00879341  0.01742913  0.00429728\n",
      "   0.0037631   0.00680255  0.02893664 -0.00067965]]\n",
      "(1, 21)\n",
      "\n",
      "POET result:\n",
      "AV: 89.6868\n",
      "SD: 7.8710\n",
      "SR: 11.3946\n"
     ]
    }
   ],
   "source": [
    "### POET estimation\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "\n",
    "print(\"generating components:\")\n",
    "pca = PCA(n_components=5, copy=True)\n",
    "pca.fit(yt_est)\n",
    "factors=pca.components_\n",
    "print('PCA Componets:\\n', pca.components_)\n",
    "print(np.shape(pca.components_))\n",
    "print('\\nEigenvalues:', pca.explained_variance_)\n",
    "print('Variance explaination:', pca.explained_variance_ratio_)\n",
    "print(\"Add up: \",np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "##Regression:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"\\nRegression:\")\n",
    "LG2=LinearRegression()\n",
    "\n",
    "LG2.fit(factors.T,yt_est.T)\n",
    "betas=LG2.coef_\n",
    "print(\"Beta matrix: \",np.shape(betas))\n",
    "\n",
    "var_fs=np.cov(factors)\n",
    "\n",
    "SigmaF=betas@var_fs@betas.T\n",
    "\n",
    "###As same as SF, the diagonal need add residual,or replace by var(Ri)\n",
    "\n",
    "for i in range(100):\n",
    "    SigmaF[i,i]=np.cov(yt_est)[i,i]\n",
    "    \n",
    "showmatrixinfo(SigmaF)\n",
    "\n",
    "resultPOET=test_result(SigmaF,\"POET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ffce80-c41b-4819-b9a2-0b4f5221dc4f",
   "metadata": {},
   "source": [
    "## NL-SF\n",
    "This is the non-linear shrinkage of single factor covariance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42cad233-b9e9-46ba-93df-c7934e2366f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 100)\n",
      "True\n",
      "w:\n",
      "[[-1.89907661e-03 -1.28758968e-02 -4.81710394e-02  2.57703894e-03\n",
      "  -1.21959323e-02 -4.17158853e-02 -5.68550584e-03  3.30552271e-02\n",
      "   2.64036090e-02  2.14386794e-03  4.96232723e-02  1.01933818e-02\n",
      "   3.53642763e-02  1.60559502e-02 -3.95555739e-02  1.27240739e-02\n",
      "   5.01171137e-02  3.77655711e-03  2.28651082e-02 -1.78437973e-02\n",
      "   7.78816877e-03 -6.20320416e-04 -1.24839867e-02  4.38014334e-03\n",
      "   1.28752218e-02 -7.86085914e-03 -1.14153421e-02 -1.88747269e-02\n",
      "  -1.23695306e-02 -3.06660976e-02  3.31117809e-02  5.38799966e-02\n",
      "   2.68658413e-02  4.53590697e-02  9.09430099e-02 -2.30662120e-02\n",
      "  -3.62776904e-02  1.43075894e-01  5.24399869e-03  9.60572434e-03\n",
      "   1.04168461e-02 -1.38864904e-02 -1.61510917e-02  2.61536825e-02\n",
      "   1.05112312e-01  1.87237319e-02  3.41758943e-02 -1.23934324e-02\n",
      "   2.95960570e-03 -1.79864149e-02 -2.01470851e-03  5.71946236e-02\n",
      "  -4.67691613e-05  1.88710593e-02 -9.98113705e-03  1.80487908e-02\n",
      "  -4.27787588e-02 -1.54216228e-02  1.86510931e-02  1.33555412e-02\n",
      "   5.50837094e-02  9.36427768e-03  3.48642193e-03  1.31976661e-01\n",
      "  -6.49226786e-02  3.41454454e-02  6.64049516e-03 -1.96391615e-02\n",
      "  -2.21255281e-02  5.11219525e-02 -1.23300532e-02 -3.42474490e-02\n",
      "  -7.00379195e-03 -1.82270066e-02  1.57721102e-02  5.57128904e-02\n",
      "   4.95742393e-02  2.47767352e-02  1.22444580e-02  3.67708543e-02\n",
      "  -1.34984536e-02 -3.09511055e-02  1.12511548e-02  3.57602850e-02\n",
      "  -2.48185727e-02  1.08743728e-02  9.85767897e-03 -1.88643543e-02\n",
      "   9.91269700e-03 -9.33513308e-03  8.20012277e-03  4.68733112e-02\n",
      "   2.10798146e-02  9.85984515e-03  5.18422004e-02  1.06878748e-02\n",
      "  -1.12698815e-03  1.15353994e-02  5.48663275e-03 -1.02549708e-02]]\n",
      "(1, 21)\n",
      "\n",
      "NL-SF result:\n",
      "AV: 73.1902\n",
      "SD: 7.7137\n",
      "SR: 9.4884\n"
     ]
    }
   ],
   "source": [
    "## NL-SF\n",
    "eigenvalue, eigenvectors = np.linalg.eig(SigmaSF)\n",
    "print(np.shape(eigenvalue))\n",
    "print(np.shape(eigenvectors))\n",
    "\n",
    "print(np.allclose(SigmaSF,eigenvectors@np.diag(eigenvalue)@eigenvectors.T))\n",
    "\n",
    "diag=np.identity(100)\n",
    "diag2=np.zeros((100,100))\n",
    "for i in range(100):\n",
    "    diag[i,i]=pow(eigenvalue[i],-1/2)\n",
    "    diag2[i,i]=pow(eigenvalue[i],1/2)\n",
    "##Generate Yt x Sigma_SF to the power of -1/2\n",
    "SigmaSF2=eigenvectors@diag@eigenvectors.T  ##(1/2)\n",
    "SigmaSF3=eigenvectors@diag2@eigenvectors.T  ##(-1/2)\n",
    "SigmaC_hat=nls.shrink_cov(yt_est.T@SigmaSF2)\n",
    "\n",
    "#Reincorporating the structure.\n",
    "SigmaNLSF=SigmaSF3@SigmaC_hat@SigmaSF3\n",
    "resultNLSF=test_result(SigmaNLSF,\"NL-SF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52ad47-a446-44a2-b089-62264dcfd5b8",
   "metadata": {},
   "source": [
    "## Final Results\n",
    "This is all the result I estimated above. I made a tab similar to the original one in Ledoit and Wolf(2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3813b83e-b116-4efc-ac64-5c4dc6436b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1\n",
      "Performance measures for various estimators of the GMV portfolio\n",
      "Period: January 19, 1973 to December 31, 2011\" \n",
      "\t1/N \tSample \tLin \tNolin \tSF \tFF \tPOET \tNL-SF\n",
      "----------------------------------------------------------------------\n",
      "                                N=100                                 \n",
      "----------------------------------------------------------------------\n",
      "AV \t54.87 \t55.60\t61.44\t73.74\t94.55\t89.11\t89.69\t73.19\n",
      "SD \t8.51 \t10.29\t8.41\t7.10\t10.0\t8.63\t7.87\t7.71\n",
      "SR \t6.45 \t5.40\t7.30\t10.38\t9.42\t10.33\t11.39\t9.49\n"
     ]
    }
   ],
   "source": [
    "#####Show all the results\n",
    "\n",
    "def prtb(result1N,resultSamp,resultLin,resultNonLin,resultSF,resultFF,resultPOET,resultNLSF):\n",
    "    print(\"Table 1\")\n",
    "    print(\"Performance measures for various estimators of the GMV portfolio\")\n",
    "    print(\"{}\".format('''Period: January 19, 1973 to December 31, 2011\" '''))\n",
    "    print(\"\\t1/N \\tSample \\tLin \\tNolin \\tSF \\tFF \\tPOET \\tNL-SF\")\n",
    "    print(\"{:-^70}\".format(\"\"))\n",
    "    print(\"{:^70}\".format(\"N=100\"))\n",
    "    print(\"{:-^70}\".format(\"\"))\n",
    "    print(\"AV \\t{:.2f} \\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(\n",
    "        result1N[0],resultSamp[0],resultLin[0],resultNonLin[0],resultSF[0],resultFF[0],resultPOET[0],resultNLSF[0]))\n",
    "    print(\"SD \\t{:.2f} \\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.1f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(\n",
    "        result1N[1],resultSamp[1],resultLin[1],resultNonLin[1],resultSF[1],resultFF[1],resultPOET[1],resultNLSF[1]))\n",
    "    print(\"SR \\t{:.2f} \\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(\n",
    "        result1N[2],resultSamp[2],resultLin[2],resultNonLin[2],resultSF[2],resultFF[2],resultPOET[2],resultNLSF[2]))\n",
    "    return\n",
    "\n",
    "\n",
    "prtb(result1N,resultSamp,resultLin,resultNonLin,resultSF,resultFF,resultPOET,resultNLSF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612def1-f57c-4272-bc3f-701ff09286c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## About this project\n",
    "In this project I replicated the table 1 of Ledoit & Wolf(2017).\n",
    "\n",
    "I mainly use csv to do the IO, and I also used some .npy files to save numpy matrix. csv files can be loaded in excel so I can use excel, stata to calculate certain numbers to check if I coded them right.\n",
    "\n",
    "I transformed all the data into matrixes and calculate them using linear algebra. I use certain packages to estimate parameters(mainly sklearn)\n",
    "\n",
    "## References\n",
    "About reference, I did all the coding myself, but I referenced a lot of materials.\n",
    "I learned numpy, pandas and sklearn through internet, including google, baidu, youtube, bilibili, and manuals of these packages.\n",
    "I also referenced the methods in the article on the internet and some textbooks.\n",
    "\n",
    "### Papers:  \n",
    "Fama, E. F. and K. R. French. 1993. Common risk factors in the returns on stocks and bonds.\n",
    "Journal of Financial Economics 33: 356.  \n",
    "\n",
    "Fan, J., Y. Liao, and M. Mincheva. 2013. Large covariance estimation by thresholding\n",
    "principal orthogonal complements (with discussion). Journal of the Royal Statistical Society,\n",
    "Series B 75: 603680.  \n",
    "\n",
    "Ledoit, O. and M. Wolf. 2004b. A well-conditioned estimator for large-dimensional covariance matrices. Journal\n",
    "of Multivariate Analysis 88: 365411. \n",
    "\n",
    "Ledoit, O. and M. Wolf. 2012. Nonlinear shrinkage estimation of large-dimensional covariance matrices. Annals\n",
    "of Statistics 40: 10241060.  \n",
    "\n",
    "Ledoit, O. and M. Wolf. 2015. Spectrum estimation: a unified framework for covariance matrix estimation and\n",
    "PCA in large dimensions. Journal of Multivariate Analysis 139: 360384\n",
    "\n",
    "Markowitz, H. 1952. Portfolio selection. Journal of Finance 7: 7791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bb351-e4a9-4cb5-97c6-0a8e64e603f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
